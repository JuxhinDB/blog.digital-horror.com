
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
    <head>
        <title>Digital Horror</title>
        <meta name="description" content="Re-exploring the Hamming Code history and implementation. Rewriting the implementation in Rust and improving its performance using neat assembly tricks.">
        <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Load light and dark themes for syntax highlighting -->
<link id="prism-light-theme" href="/themes/prism-theme-github-light.css" rel="stylesheet" />
<link id="prism-dark-theme" href="/themes/prism-theme-github-dark.css" media="none" rel="stylesheet" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.27.0/prism.min.js"></script>

<!-- Load light and dark themes for overall layout -->
<link class="main-stylesheet" id="light-mode" rel="stylesheet" href="/css/styles.css">
<link class="main-stylesheet" id="dark-mode" rel="stylesheet" media="none" href="/css/styles-dark.css" />

<!-- Add webfont support -->
<link rel="stylesheet" href="/fonts/icons.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@mdi/font@latest/css/materialdesignicons.min.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato&family=Merriweather&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">

<script>
    function waitForElem(selector) {
        return new Promise(resolve => {
            if (document.querySelector(selector)) {
                console.log('element already exists');
                return resolve(document.querySelector(selector));
            }
    
            const observer = new MutationObserver(mutations => {
                if (document.querySelector(selector)) {
                    console.log('found element ' + selector);
                    resolve(document.querySelector(selector));
                    observer.disconnect();
                }
            });
    
            observer.observe(document.body, {
                  childList: true
                , subtree: true
                , attributes: false
                , characterData: false
            });
        });
    }
</script>

    </head>
    <body>
    <div class="container content-container">
        <header>
   <div class="header-content">
       <div class="header-center">
           <a href="/">Digital Horror</a>
       </div>
       <div class="header-right">
           <span id="dark-mode-toggle" class="mdi mdi-theme-light-dark"></span>
       </div>
   </div>
</header>



        <main>
            <article class="post">
            <h2 class="post-title">Rediscovering Hamming Code</h2>
            <p class="post-date">18/04/2021</p>
                <div class = "post-content"> 
                <blockquote>
<p><strong>Note</strong> â€” if you would like take a closer look at the project, you can find it <a href="https://github.com/JuxhinDB/hamming-code">here</a>.</p>
</blockquote>
<p>A couple of months back I had started reading an interesting book by the late Richard Hamming, &quot;<a href="https://www.goodreads.com/book/show/530415.The_Art_of_Doing_Science_and_Engineering">The Art of Doing Science and Engineering: Learning to Learn</a>&quot;.</p>
<p>It's a good book but we only get one chapter on hamming code (and four on <a href="https://en.wikipedia.org/wiki/Digital_filter%22%3Edigital">digital filters</a>); earlier chapters however are definitely word a read. Really and truly I hardly understood most it but I did love the narrative on learning, how to learn, and the story on how the hamming code was discovered.</p>
<p>Since my interest in digital filters was, and is, somewhat non-existent I decided hamming code would be a great place to learn a couple of interesting topics and tools for a few reasons:</p>
<ul>
<li>It's a well defined problem domain (no room for interpretation);</li>
<li>Multiple components with some shared dependency between components;</li>
<li>Small but not too small of an algorithm to optimise and learn from.</li>
</ul>
<p>With these characteristics I wanted to explore a few areas around profiling, benchmarking, assembly, concurrency, SIMD and overall bit twiddling all in one place and done gradually.</p>
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="header-introduction"></a>Introduction</h2>
<p>I will spare the detailed explanation of Hamming code as there is already very work out there, as well as different ways of approaching the problem depending on hardware and software. That said, for both of these cases I highly recommend two videos by <a href="https://www.youtube.com/watch?v=X8jsijhllIA">3Blue1Brown</a> and <a href="https://www.youtube.com/watch?v=h0jloehRKas&amp;t=0s">Ben Eater</a>.</p>
<div class="youtube-embedded-container">
    <iframe width="100%" height="100%" src="https://www.youtube.com/embed/X8jsijhllIA" title="How to send a self-correcting message (Hamming codes)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<p>In this particular task I want to be able to do one thing and do it fast. Encode, add noise and decode binary files (yes, technically it's three things). Here are some initial versions trying to encode &amp; decode a lorem ipsum text file.</p>
<p><img src="../res/rediscovering-hamming-code/lorem-ipsum.png" alt="Corrupted Lorem Ipsum text." /></p>
<p>Well, at least it's roughly equal amount of gibberish. Maybe I got the byte slices all wrong?</p>
<p><img src="../res/rediscovering-hamming-code/image-11.png" alt="Corrupted Lorem Ipsum text." /></p>
<p>Seems like I did but instead of fixing it, I made it worse. Some alignment issues, algorithm issues and endianness issues later we managed to get some basic text encoding &amp; decoding working that would translate well to any binary files (in theory).</p>
<p><img src="../res/rediscovering-hamming-code/image-12.png" alt="Corrupted Lorem Ipsum text." /></p>
<p>Time to apply some synthetic noise on a per-block basis with a 66.67% chance of flipping a single bit. In real world scenarios binary blocks are interleaved to reduce chances of single blocks have more than 1 bit error which Hamming cannot handle.</p>
<blockquote>
<p>The reason for this is that when a bit is damaged in transmission, it tends to occur in a short burst were a few adjacent bits are all flipped altogether. Interleaving blocks builds resiliency against such scenarios.</p>
</blockquote>
<pre><code class="language-none">Block:   0000000001100001011100100111010001100101011100100110000101101000
Encoded: 0011000010111001001110100011001001011100100110000010110110010111
Decoded: 0000000001100001011100100111010001100101011100100110000101101000
Match?   true

Block:   0000000001100001011010010110011101110101011001010110011000100000
Encoded: 0011000010110100101100111011101001011001010110011100010000010011
flipping bit: 12

error at bit: 12
Decoded: 0000000001100001011010010110011101110101011001010110011000100000
Match?   true
</code></pre>
<p>I tried my hand at some images and somehow made a cursed cat look less terrifying. It's a feature not a bug.</p>
<p><img src="../res/rediscovering-hamming-code/cursed-cat-2.png" alt="Cat partially corrected." /></p>
<p>As it turns out, my file reader is only able to process up to, and exactly, 7168 bytes? I decided to not worry about this for now and move on; I'd be more than happy to feedback on alternative ways to write <a href="https://github.com/JuxhinDB/hamming-code/blob/main/src/bin/main.rs">this</a>.</p>
<h2><a href="#implementation" aria-hidden="true" class="anchor" id="header-implementation"></a>Implementation</h2>
<p>It took some back-of-the-napkin dry runs till I got the crux of the problem and looking at <a href="https://github.com/tckmn/rustyham/blob/master/src/lib.rs">other similar implementations</a>, we narrowed down to something that worked. I wanted something simple, that can be optimised so we landed for the following <a href="https://github.com/JuxhinDB/hamming-code/tree/57e6a63ed75b85dcda82b5ec6f8a9059da300917">initial implementation</a>.</p>
<p>We will be going over each component, (<strong>a</strong>) encoder, (<strong>b</strong>) decoder and (<strong>c</strong>) parity check independently to avoid context switching too much and streamlining the improvements.</p>
<h2><a href="#encoder" aria-hidden="true" class="anchor" id="header-encoder"></a>Encoder</h2>
<p>The initial encoder is a little bit allover the place but is arguably the simplest part of this implementation. We need to figure how many parity bits, len_power, we need (log2{64 bits} = 6) and from that derive the data width, len (2^6 = 64); you will notice soon enough that this entire dance is not required.</p>
<pre><code class="language-rust">pub fn encode(block: &amp;mut u64) -&gt; u64 {
    let len_power = (2..).find(|&amp;r| 2u32.pow(r) - r - 1 &gt;= 32).unwrap();
    let len = 2usize.pow(len_power);
    // ...

}
</code></pre>
<p>These are the only parameters that the encoder needs to be aware of before we can start to encode our message. Starting with <code>1001</code> as our example input message, we first go through each bit, see if the index of that bit is a power of 2 and therefore a parity bit. If it is a power of 2, we shift the input message left (i.e. skip the current bit), otherwise we insert the bit into the encoded message as is.</p>
<pre><code class="language-rust">pub fn encode(block: &amp;mut u64) -&gt; u64 {
    // ...
    let mut code = 0u64;

    for i in 0..len {
        // Check if `i` is not a power of 2
        if (i != 0) &amp;&amp; (i &amp; (i - 1)) != 0 {
            code |= (0b1 &lt;&lt; i) &amp; *block as u64;
        } else {
            *block &lt;&lt;= 1;
        }
    }
    // ...
}
</code></pre>
<p>If we were to think of this graphically, it may look something like this, (1) we get our raw input, (2) we map the raw input to the encoded output as part of its data bits and (3) we calculate the parity bits and map them to the encoded output as parity bits.</p>
<p><img src="../res/rediscovering-hamming-code/hamming-code-diagrams-encoding.png" alt="Encoding a block." /></p>
<p>Let's test the implementation out; the results match the expected output and running through a simple benchmark (<a href="https://github.com/JuxhinDB/hamming-code/blob/acaf4b9063b2b80a95b38f18dd173104a21d6c9f/benches/benchmark.rs">criterion</a>), does not yield promising results. Our baseline median execution time of <strong>430.44n</strong> for our encoder with a large number of odd outliers. We now have something that we can improve.</p>
<p><img src="../res/rediscovering-hamming-code/pdf-1.png" alt="Encoding benchmark." /></p>
<p>There are some definite quick wins we can work with here. For starters, since we know we're always going to work with u64 (or any fixed-width block size), len_power and len do not need to be computed every time we want to encode a block. Let's also remove the <a href="https://doc.rust-lang.org/std/marker/trait.Copy.html"><code>code</code></a> for the internal encoded variable that is not needed.</p>
<pre><code class="language-rust">pub fn encode(block: &amp;mut u64) -&gt; u64 {
    let len_power = 6;
    let len = 64;

    let mut code = 0u64;
	
    // ... Remove `encoded` and work directly on `code` only

    for i in 0..len_power {
        // If the parity check is odd, set the bit to 1 otherwise move on.
        if !parity(&amp;code, i) {
            code |= 0b1 &lt;&lt; (2usize.pow(i) - 1);
        }
    }

    code
}
</code></pre>
<p>Re-running our benchmarks, we now run in a median time of 361.27ns, a ~17.5% performance increase. We're still seeing some outliers, but the average execution time is now bundled up better.</p>
<p><img src="../res/rediscovering-hamming-code/pdf--1--1.png" alt="Encoding benchmark." /></p>
<p>At this stage, there are two branching conditions left in our encoder which I will largely leave as to avoid over optimising early on. Let's profile (via <a href="https://en.wikipedia.org/wiki/Perf_(Linux)"><code>perf</code></a>) and plot (via <a href="https://github.com/flamegraph-rs/flamegraph"><code>flamegraph</code></a>) our encoder and take a peak at the overall execution time.</p>
<blockquote>
<p>This SVG was hand-edited to save space and keep it interactive. It was not fun and the sample frequency in this sample is low but it translates well enough to larger sample rates (~99Hz).</p>
</blockquote>
<div class="img-container">
    <embed style="width:100%" src="/res/rediscovering-hamming-code/flamegraph.svg">
</div>
<p>The primary culprit here is the parity function taking circa ~75% of the total encoder execution time. It's clear that our parity function isn't really doing too great so the next natural step is to optimise it which will positively affect the entire implementation. Referring back to the introduction, this is what I meant by shared component.</p>
<h2><a href="#parity-check" aria-hidden="true" class="anchor" id="header-parity-check"></a>Parity Check</h2>
<p>Let's take a boiled down view of the implementation to understand where and how we can optimise our implementation. The parity checker's job is to determine if a certain sequence of bits are either odd (1), or even (0). Recall our encoded message is made up of data bits and parity bits.</p>
<p><img src="../res/rediscovering-hamming-code/hamming-code-diagrams-parity.png" alt="Parity check diagraam." /></p>
<p>If we actually take a look at the indexes of the parity bits, they are always exactly a power of 2. So our set of parity bits P can be rewritten from <code>P = { P1, P2, P3, P4 }</code> to <code>P = { 0001, 0010, 0100, 1000 }</code> whereby an intuitive and elegant pattern emerges.</p>
<ul>
<li>p1 <code>(0001)</code> is checking for <em>all values</em> that have 1 as their <strong>firs</strong> bit;</li>
<li>p2 <code>(0010)</code> is checking for <em>all values</em> that have 1 as their <strong>secon</strong> bit;</li>
<li>p3 <code>(0100)</code> is checking for <em>all values</em> that have 1 as their <strong>thir</strong> bit;</li>
<li>p4 <code>(1000)</code> is checking for <em>all values</em> that have 1 as their <strong>fourt</strong> bit.</li>
</ul>
<p><code>P1 = 2^0 = 1</code> therefore checks for 1 bit, then skips 1 bit. <code>P2 = 2^1 = 2</code> checks for 2 bits, then skips two bits. <code>P3</code> checks every 4 bits then skips 4 bits and finally <code>P4</code> checks every 8 bits then skips 8 bits.</p>
<blockquote>
<p><strong>Not</strong> â€” ignore the <em>0th</em> bit for now. That will be a global parity bit, part of extended hamming code that checks parity <strong>al</strong> bits. We'll get back to this.</p>
</blockquote>
<p>This visualiation helps to show how parity bits are spread and interleaved to maximise their coverage of all the binary block.</p>
<p><img src="../res/rediscovering-hamming-code/hamming-code-diagrams-parity-1.png" alt="Parity check diagraam." /></p>
<p>There are many cases where we do not need to check every bit. For example.</p>
<ul>
<li>Parity bit <code>P4</code> starts from the 8th element, so there is no need to check the first 8 bits (we may jump ahead);</li>
<li>Do not iterate over every bit incrementally, instead skip over bits that are not relevant to a given parity bit. If <code>P2</code> checks of <code>0010</code> and <code>0011</code> and skips <code>0100</code> and <code>0101</code>, then there is no need to iterate over the latter two.</li>
</ul>
<p>Let's get to our first implementation first and see how we may improve it. Taking a look at our parity check, it may look more involved relative to our encoder.  If we compare it to the points above, we're doing a good job of not starting the loop from the 0th index but rather from the first bit that is relevant for a given parity. One thing we are not doing so well is that this implementation goes over every bit and then determines if we should ignore it or not, which is costly.</p>
<pre><code class="language-rust">fn parity(code: &amp;u64, i: u32) -&gt; bool {
    let bi = (0b1 &lt;&lt; i) - 1;
    let (mut parity, mut ignore, mut counter) = (true, false, 0);
    for j in bi..64 {
        if !ignore &amp;&amp; (code &amp; 0b1 &lt;&lt; j) != 0b0 {
            parity = !parity;
        }

        counter += 1;
        if counter &gt;= 0b1 &lt;&lt; i {
            ignore = !ignore;
            counter = 0;

        }
    }

    parity  // true if even
}
</code></pre>
<p>Taking a look at the underlying assembly (via cargo-asm with rust flag enabled) we see that there's a lot going on, primarily because of all the counter and ignore checks that we mention above.</p>
<pre><code class="language-rust">pub fn parity(code: &amp;u64, i: u32) -&gt; bool {
 mov     w9, #1
 let bi = (0b1 &lt;&lt; i) - 1;

 lsl     w8, w9, w1
 sub     w10, w8, #1
     cmp     w10, #63
     b.gt    LBB2_3
     mov     w12, #0
     mov     w10, #0
     ldr     x11, [x0]
     sub     w13, w8, #2
     mov     w9, #1
LBB2_2:
     add     w13, w13, #1
 if !ignore &amp;&amp; (code &amp; 0b1 &lt;&lt; j) != 0b0 {

 lsr     x14, x11, x13
 and     w14, w14, #0x1
 eor     w14, w9, w14
 tst     w12, #0x1

 csel    w9, w9, w14, ne
 counter += 1;
 add     w14, w10, #1
 if counter &gt;= 0b1 &lt;&lt; i {
 cmp     w14, w8

 cset    w14, lt
 if counter &gt;= 0b1 &lt;&lt; i {
 csinc   w10, wzr, w10, ge

 eor     w12, w14, w12
 eor     w12, w12, #0x1
     cmp     w13, #63
     b.lt    LBB2_2
LBB2_3:
 }
 and     w0, w9, #0x1
 ret
</code></pre>
<p>My initial impression is that there is not a lot of <a href="https://releases.llvm.org/2.7/docs/LangRef.html#i_br">branching</a> going on but our loops are simply longer than they need to be. Let's see if we can write the skipping functionality that actually skips bits it doesn't need to compute (as opposed to computing, then deciding if it should skip). After some back-of-the-napkin dry runs, I boiled it down to the following which passes the tests.</p>
<pre><code class="language-rust">pub fn parity(code: &amp;u64, i: u32) -&gt; bool {
    let mut parity = true;
    let spread = 2u32.pow(i);
    let mut j = spread;

    while j &lt; 64 - spread + 1 {
        for k in 0..spread {
            if (code &amp; 0b1 &lt;&lt; j + k) != 0b0 {
                parity = !parity;
            }
        }

        j += 2 * spread;
    }

    parity
}
</code></pre>
<p>We do away with the <code>ignore</code> and <code>counter</code> variables and instead skip bits depending on the parity bit we are checking. Whenever we land on a block of bits that we should be checking, we run an internal loop to scan through that entire block, dictated by some spread (i.e. number of consecutive bits we ought to compute) and subsequently jump to the next index.</p>
<p><img src="../res/rediscovering-hamming-code/hamming-code-diagrams-parity-p2-1.png" alt="Parity check diagraam." /></p>
<p>The emitted assembly is only slightly reduced but is simpler and quicker resulting in a median execution time of <strong>3.76n</strong> versus <strong>72.23ns</strong>.</p>
<p>Remember that 0th bit that we did not cover with our current parity checks? That's a special parity bit that checks the parity of the entire block (including all parity and data bits). This is known as <a href="https://en.wikipedia.org/wiki/Hamming_code#Hamming_codes_with_additional_parity_(SECDED)">extended hamming</a> code; on top of being able to correct single bit errors, it also allows us to detect (but not correct) two-bit errors. Let's try to achieve this using a naive implementation which goes over each bit and computes the global parity.</p>
<pre><code class="language-rust">pub fn slow_parity(code: u64) -&gt; bool {
    let mut parity = true;

    for i in 0..63 {
        if code &amp; 0b1 &lt;&lt; i != 0 {
            parity = !parity;
        }
    }

    parity
}
</code></pre>
<p>Running it through our benchmark we can see this takes <strong>7.7907n</strong> median execution time â€“ not great.</p>
<pre><code class="language-none">slow_parity check       time:   [7.7806 ns 7.7907 ns 7.8013 ns]                               
Found 12 outliers among 100 measurements (12.00%)
  8 (8.00%) high mild
  4 (4.00%) high severe

</code></pre>
<p>There is a more intuitive way of tackling this which I took from the book, <a href="https://www.goodreads.com/book/show/276079.Hacker_s_Delight">Hacker's Delight</a> (vol. 2, p. 96) by  Henry S. Warren Jr. If you haven't come across this book before, I really cannot recommend it enough. It's a goldmine. Here's an excerpt of the relevant section.</p>
<p><img src="../res/rediscovering-hamming-code/20210411_1901108.png" alt="Parity check diagraam." /></p>
<p>Let's implement this, performing a rolling XOR and take the rightmost bit, extended to 64-bit blocks instead of the author's original 32-bit block.</p>
<pre><code class="language-rust">pub fn fast_parity(code: u64) -&gt; u64 {
    let mut y: u64 = code ^ (code &gt;&gt; 1);

    y ^= y &gt;&gt; 2;
    y ^= y &gt;&gt; 4;

    y ^= y &gt;&gt; 8;
    y ^= y &gt;&gt; 16;
    y ^= y &gt;&gt; 32;

    0b1 &amp; y
}
</code></pre>
<p>The difference is quite stark. Running it through our benchmark we can see the fast parity check running at <strong>937p</strong> (where 1ps = 0.001ns) median execution time versus <strong>7.7907n</strong> median execution time, translating to roughly a ~8300% improvement.</p>
<pre><code class="language-none">fast_parity check       time:   [937.54 ps 937.86 ps 938.26 ps]                               
Found 13 outliers among 100 measurements (13.00%)
  5 (5.00%) high mild
  8 (8.00%) high severe

slow_parity check       time:   [7.7806 ns 7.7907 ns 7.8013 ns]                               
Found 12 outliers among 100 measurements (12.00%)
  8 (8.00%) high mild
  4 (4.00%) high severe
</code></pre>
<p>If we compare the assembly code generated we can take a peek as to how this result is achieved thanks to compiler optimisations.</p>
<h4><a href="#assembly-arm--fast-parity" aria-hidden="true" class="anchor" id="header-assembly-arm--fast-parity"></a>Assembly (ARM) â€” Fast parity</h4>
<pre><code class="language-asm">hamming_code_simd::hamming::fast_parity (src/hamming.rs:66):
 eor     x8, x0, x0, lsr, #1
 eor     x8, x8, x8, lsr, #2
 eor     x8, x8, x8, lsr, #4

 eor     x8, x8, x8, lsr, #8
 eor     x8, x8, x8, lsr, #16
 lsr     x9, x8, #32
 eor     w8, w9, w8
 and     x0, x8, #0x1
 ret
</code></pre>
<p>We can now easily use the fast_parity check in our encoder before returning the encoded message at the very bottom.</p>
<pre><code class="language-rust">code |= fast_parity(code);
</code></pre>
<h3><a href="#benchmark" aria-hidden="true" class="anchor" id="header-benchmark"></a>Benchmark</h3>
<p>Running our benchmarks one more time for both the encoder and decoder yields some promising improvements.</p>
<pre><code class="language-none">hamming encode time:   [153.33 ns 153.38 ns 153.43 ns]                           
                        change: [-57.763% -57.720% -57.680%] (p = 0.00 &lt; 0.05)
                        Performance has improved.

hamming decode time:   [118.57 ns 118.60 ns 118.65 ns]                           
                        change: [-72.866% -72.823% -72.782%] (p = 0.00 &lt; 0.05)
                        Performance has improved.
</code></pre>
<p>We improved the encoder's performance by <b>57.72%</b> (~153ns) and decoder by <b>72.82%</b> (~118.6ns).</p>
<blockquote>
<p>These results are achieved on ARM and results will vary on x86.</p>
</blockquote>
<p><img src="../res/rediscovering-hamming-code/pdf-6-1.png" alt="Parity check diagram." /></p>
<p>If we contrast this with our previous implementations we can see a much faster execution time as well as more stability (dictated by density) with fewer outliers.</p>
<p><img src="../res/rediscovering-hamming-code/pdf-7-1.png" alt="Parity check diagram." /></p>
<p>At this stage, this should cover most of the improvements from an algorithm perspective. There may be some room to further optimise the overall parity checker (not the fast global one) but we can get to that later.</p>
<h2><a href="#decoder" aria-hidden="true" class="anchor" id="header-decoder"></a>Decoder</h2>
<p>The remaining part of our implementation is the decoder. I will not go into as much detail as the implementation is relatively similar.</p>
<ul>
<li>Re-calculate every parity bit, if there are any odd parities then we have at least one error;</li>
<li>Accumulate all the incorrect parities into a single error &quot;check&quot; value;</li>
<li>If the &quot;check&quot; value is greater than 0, we have an error. Flip the bit corresponding to the &quot;check&quot; value (e.g. if error check is 10110, flip the bit in the 11th index);</li>
<li>Drop all parity bits and compress all data bits together.</li>
</ul>
<pre><code class="language-rust">pub fn decode(code: &amp;mut u64) -&gt; u64 {
    let len_power = 6;
    let len = 64;

    let mut check = 0b0;

    for i in 0..len_power {
        if !parity(&amp;code, i) {
            check |= 0b1 &lt;&lt; i;
        }
    }

    // We have an error
    if check &gt; 0b0 {
        *code ^= 0b1 &lt;&lt; check;
    }

    // Drop all parity bits
    let mut offset = 0;
    let mut decoded = 0b0;

    for i in 0..len {
        // Check if `i` is not a power of 2
        if (i != 0) &amp;&amp; (i &amp; (i - 1)) != 0 {
            decoded |= ((0b1 &lt;&lt; i) &amp; *code) &gt;&gt; offset;
        } else {
            offset += 1;
        }
    }


    decoded
}
</code></pre>
<p>There isn't much else left to add here but I'll still be exploring some ways to speed this up. Keep in mind that right now there is no global parity check.</p>
<h2><a href="#next-steps" aria-hidden="true" class="anchor" id="header-next-steps"></a>Next steps</h2>
<p>This post took a while to write but it aims to show how one might go about tackling a problem and later optimising it. The implementation is not feature complete, there are a few boundary checks missing and we are not checking and testing for the global parity bit in the decoder â€“ it's meant to be a gradual learning exercise.</p>
<p>I am contemplating extending this solution to optimise it from a workload distribution standpoint through <a href="https://doc.rust-lang.org/edition-guide/rust-2018/simd-for-faster-computing.html">SIMD</a> and/or parallel workloads. What happens if we use narrower or wider data blocks (i.e. 256-bit)? Can we calculate how many bytes by second we can process with this encoder using real-world scenarios? What's the expected industry standard and can we get close to (perhaps even leveraging architecture specific instructions).</p>
<p>If you are interested in this or have any feedback you'd like to pass, feel free to <a href="https://twitter.com/juxhindb">reach out</a>, I'd love to hear your input.</p>

                </div>
            </article>
        </main>
    </div>

    <footer>
   <div class="footer-content">
       <div class="footer-center">
           &copy; 2023 Digital Horror. All rights reserved.
       </div>
       <div class="footer-right">
           <a href="https://linkedin.com/in/juxhin-db" target="_blank" rel="noopener noreferrer" class="social-icon">
               <span class="mdi mdi-linkedin"></span>
           </a>
           <a href="https://twitter.com/juxhindb" target="_blank" rel="noopener noreferrer" class="social-icon">
               <span class="mdi mdi-twitter"></span>
           </a>
           <a href="https://github.com/juxhindb" target="_blank" rel="noopener noreferrer" class="social-icon">
               <span class="mdi mdi-github"></span>
           </a>
       </div>
   </div>
</footer>



    <script src="/js/script.js" defer></script>
    <script src="/js/prism.js" defer></script>

    </body>
</html>
        